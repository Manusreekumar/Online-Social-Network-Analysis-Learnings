{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "# # CS579: Assignment 1\n",
    "#\n",
    "# In this assignment, we'll implement community detection and link prediction algorithms using Facebook \"like\" data.\n",
    "#\n",
    "# The file `edges.txt.gz` indicates like relationships between facebook users. This was collected using snowball sampling: beginning with the user \"Bill Gates\", I crawled all the people he \"likes\", then, for each newly discovered user, I crawled all the people they liked.\n",
    "#\n",
    "# We'll cluster the resulting graph into communities, as well as recommend friends for Bill Gates.\n",
    "#\n",
    "# Complete the **15** methods below that are indicated by `TODO`. I've provided some sample output to help guide your implementation.\n",
    "\n",
    "\n",
    "# You should not use any imports not listed here:\n",
    "from collections import Counter, defaultdict, deque\n",
    "import copy\n",
    "import math\n",
    "import networkx as nx\n",
    "import urllib.request\n",
    "%matplotlib inline\n",
    "\n",
    "## Community Detection\n",
    "\n",
    "def example_graph():\n",
    "    g = nx.Graph()\n",
    "    g.add_edges_from([('A', 'B'), ('A', 'C'), ('B', 'C'), ('B', 'D'), ('D', 'E'), ('D', 'F'), ('D', 'G'), ('E', 'F'), ('G', 'F')])\n",
    "    return g\n",
    "\n",
    "def bfs(graph, root, max_depth):\n",
    "    \"\"\"\n",
    "    Perform breadth-first search to compute the shortest paths from a root node to all\n",
    "    other nodes in the graph. To reduce running time, the max_depth parameter ends\n",
    "    the search after the specified depth.\n",
    "    E.g., if max_depth=2, only paths of length 2 or less will be considered.\n",
    "    This means that nodes greather than max_depth distance from the root will not\n",
    "    appear in the result.\n",
    "\n",
    "    You may use these two classes to help with this implementation:\n",
    "      https://docs.python.org/3.5/library/collections.html#collections.defaultdict\n",
    "      https://docs.python.org/3.5/library/collections.html#collections.deque\n",
    "\n",
    "    Params:\n",
    "      graph.......A networkx Graph\n",
    "      root........The root node in the search graph (a string). We are computing\n",
    "                  shortest paths from this node to all others.\n",
    "      max_depth...An integer representing the maximum depth to search.\n",
    "\n",
    "    Returns:\n",
    "      node2distances...dict from each node to the length of the shortest path from\n",
    "                       the root node\n",
    "      node2num_paths...dict from each node to the number of shortest paths from \n",
    "                       the root node to this node.\n",
    "      node2parents.....dict from each node to the list of its parents in the search\n",
    "                       tree\n",
    "\n",
    "    In the doctests below, we first try with max_depth=5, then max_depth=2.\n",
    "\n",
    "    >>> node2distances, node2num_paths, node2parents = bfs(example_graph(), 'E', 5)\n",
    "    >>> sorted(node2distances.items())\n",
    "    [('A', 3), ('B', 2), ('C', 3), ('D', 1), ('E', 0), ('F', 1), ('G', 2)]\n",
    "    >>> sorted(node2num_paths.items())\n",
    "    [('A', 1), ('B', 1), ('C', 1), ('D', 1), ('E', 1), ('F', 1), ('G', 2)]\n",
    "    >>> sorted((node, sorted(parents)) for node, parents in node2parents.items())\n",
    "    [('A', ['B']), ('B', ['D']), ('C', ['B']), ('D', ['E']), ('F', ['E']), ('G', ['D', 'F'])]\n",
    "    >>> node2distances, node2num_paths, node2parents = bfs(example_graph(), 'E', 2)\n",
    "    >>> sorted(node2distances.items())\n",
    "    [('B', 2), ('D', 1), ('E', 0), ('F', 1), ('G', 2)]\n",
    "    >>> sorted(node2num_paths.items())\n",
    "    [('B', 1), ('D', 1), ('E', 1), ('F', 1), ('G', 2)]\n",
    "    >>> sorted((node, sorted(parents)) for node, parents in node2parents.items())\n",
    "    [('B', ['D']), ('D', ['E']), ('F', ['E']), ('G', ['D', 'F'])]\n",
    "    \"\"\"\n",
    "    ###TODO\n",
    "    q = deque()\n",
    "    q.append(root)\n",
    "    seen = set()\n",
    "\n",
    "    node2distances = {}\n",
    "    node2num_paths = {}\n",
    "    node2parents = defaultdict(list)\n",
    "    levelcount = 0\n",
    "    neighbourcount = 0\n",
    "    res = []\n",
    "    while ((len(q) > 0) and levelcount <max_depth):\n",
    "        n = q.popleft()\n",
    "        if (n == root):\n",
    "            node2distances[n] =0\n",
    "            node2num_paths[n] = 1\n",
    "        if n not in seen:\n",
    "            \n",
    "            seen.add(n)\n",
    "            res.append(n)\n",
    "            \n",
    "        \n",
    "        for nn in graph.neighbors(n):\n",
    "            if nn not in seen:\n",
    "                q.append(nn)\n",
    "                \n",
    "                \n",
    "                if (nn not in node2distances):\n",
    "                    \n",
    "                    node2distances[nn] = node2distances[n] + 1\n",
    "            if ((node2distances[nn] > node2distances[n]) and (n not in node2parents[nn])) :\n",
    "                node2parents[nn].append(n)\n",
    "                node2num_paths[nn] = len(node2parents[nn])\n",
    "        if len(q) > 0:\n",
    "            levelcount = node2distances[q[0]]\n",
    "    return node2distances ,node2num_paths, node2parents\n",
    "\n",
    "\n",
    "def complexity_of_bfs(V, E, K):\n",
    "    \"\"\"\n",
    "    If V is the number of vertices in a graph, E is the number of\n",
    "    edges, and K is the max_depth of our approximate breadth-first\n",
    "    search algorithm, then what is the *worst-case* run-time of\n",
    "    this algorithm? As usual in complexity analysis, you can ignore\n",
    "    any constant factors. E.g., if you think the answer is 2V * E + 3log(K),\n",
    "    you would return V * E + math.log(K)\n",
    "    >>> v = complexity_of_bfs(13, 23, 7)\n",
    "    >>> type(v) == int or type(v) == float\n",
    "    True\n",
    "    \"\"\"\n",
    "    ###TODO\n",
    "    return ( E * V)\n",
    "\n",
    "def bottom_up(root, node2distances, node2num_paths, node2parents):\n",
    "    \"\"\"\n",
    "    Compute the final step of the Girvan-Newman algorithm.\n",
    "    See p 352 From your text:\n",
    "    https://github.com/iit-cs579/main/blob/master/read/lru-10.pdf\n",
    "        The third and final step is to calculate for each edge e the sum\n",
    "        over all nodes Y of the fraction of shortest paths from the root\n",
    "        X to Y that go through e. This calculation involves computing this\n",
    "        sum for both nodes and edges, from the bottom. Each node other\n",
    "        than the root is given a credit of 1, representing the shortest\n",
    "        path to that node. This credit may be divided among nodes and\n",
    "        edges above, since there could be several different shortest paths\n",
    "        to the node. The rules for the calculation are as follows: ...\n",
    "\n",
    "    Params:\n",
    "      root.............The root node in the search graph (a string). We are computing\n",
    "                       shortest paths from this node to all others.\n",
    "      node2distances...dict from each node to the length of the shortest path from\n",
    "                       the root node\n",
    "      node2num_paths...dict from each node to the number of shortest paths from the\n",
    "                       root node that pass through this node.\n",
    "      node2parents.....dict from each node to the list of its parents in the search\n",
    "                       tree\n",
    "    Returns:\n",
    "      A dict mapping edges to credit value. Each key is a tuple of two strings\n",
    "      representing an edge (e.g., ('A', 'B')). Make sure each of these tuples\n",
    "      are sorted alphabetically (so, it's ('A', 'B'), not ('B', 'A')).\n",
    "\n",
    "      Any edges excluded from the results in bfs should also be exluded here.\n",
    "\n",
    "    >>> node2distances, node2num_paths, node2parents = bfs(example_graph(), 'E', 5)\n",
    "    >>> result = bottom_up('E', node2distances, node2num_paths, node2parents)\n",
    "    >>> sorted(result.items())\n",
    "    [(('A', 'B'), 1.0), (('B', 'C'), 1.0), (('B', 'D'), 3.0), (('D', 'E'), 4.5), (('D', 'G'), 0.5), (('E', 'F'), 1.5), (('F', 'G'), 0.5)]\n",
    "    \"\"\"\n",
    "    ###TODO\n",
    "    finalscore = {}\n",
    "    credit = {}\n",
    "    \n",
    "    for n , dist in sorted(node2distances.items(), key = lambda x :-x[1] ):\n",
    "       \n",
    "        denom = 0\n",
    "\n",
    "        if n not in credit:\n",
    "            credit[n] = 1\n",
    "        for nn in node2parents[n]:\n",
    "            if nn not in credit:\n",
    "                credit[nn] = 1\n",
    "            denom = denom + node2num_paths[nn]\n",
    "        for nn in node2parents[n]:\n",
    "            if ( n > nn):\n",
    "                tuplekey = (nn, n)\n",
    "            else:\n",
    "                tuplekey = (n,nn)\n",
    "        \n",
    "            finalscore[tuplekey] = (node2num_paths[nn] / denom) * credit[n]\n",
    "            credit[nn] = credit[nn] + finalscore[tuplekey]\n",
    "            \n",
    "    #for n in finalscore.keys():\n",
    "     #   finalscore[n] = finalscore[n] / 2\n",
    "    return (finalscore)\n",
    "        \n",
    "            \n",
    "\n",
    "def approximate_betweenness(graph, max_depth):\n",
    "    \"\"\"\n",
    "    Compute the approximate betweenness of each edge, using max_depth to reduce\n",
    "    computation time in breadth-first search.\n",
    "\n",
    "    You should call the bfs and bottom_up functions defined above for each node\n",
    "    in the graph, and sum together the results. Be sure to divide by 2 at the\n",
    "    end to get the final betweenness.\n",
    "\n",
    "    Params:\n",
    "      graph.......A networkx Graph\n",
    "      max_depth...An integer representing the maximum depth to search.\n",
    "\n",
    "    Returns:\n",
    "      A dict mapping edges to betweenness. Each key is a tuple of two strings\n",
    "      representing an edge (e.g., ('A', 'B')). Make sure each of these tuples\n",
    "      are sorted alphabetically (so, it's ('A', 'B'), not ('B', 'A')).\n",
    "    \n",
    "    >>> sorted(approximate_betweenness(example_graph(), 2).items())\n",
    "    [(('A', 'B'), 2.0), (('A', 'C'), 1.0), (('B', 'C'), 2.0), (('B', 'D'), 6.0), (('D', 'E'), 2.5), (('D', 'F'), 2.0), (('D', 'G'), 2.5), (('E', 'F'), 1.5), (('F', 'G'), 1.5)]\n",
    "    \"\"\"\n",
    "    ###TODO\n",
    "    finalresult = Counter()\n",
    "    for node in graph:\n",
    "        node2distances, node2num_paths, node2parents = bfs(graph, node, max_depth)\n",
    "        finalresult = finalresult + Counter(bottom_up(node, node2distances, node2num_paths, node2parents))\n",
    "    for n in finalresult.keys():\n",
    "        finalresult[n] = finalresult[n] / 2\n",
    "    return(dict(finalresult))\n",
    "        \n",
    "\n",
    "def is_approximation_always_right():\n",
    "    \"\"\"\n",
    "    Look at the doctests for approximate betweenness. In this example, the\n",
    "    edge with the highest betweenness was ('B', 'D') for both cases (when\n",
    "    max_depth=5 and max_depth=2).\n",
    "\n",
    "    Consider an arbitrary graph G. For all max_depth > 1, will it always be\n",
    "    the case that the edge with the highest betweenness will be the same\n",
    "    using either approximate_betweenness verses the exact computation?\n",
    "    Answer this question below.\n",
    "    \n",
    "    In this function, you just need to return either the string 'yes' or 'no'\n",
    "    (no need to do any actual computations here).\n",
    "    >>> s = is_approximation_always_right()\n",
    "    >>> type(s)\n",
    "    <class 'str'>\n",
    "    \"\"\"\n",
    "    ###TODO\n",
    "    return('no')\n",
    "\n",
    "def partition_girvan_newman(graph, max_depth):\n",
    "    \"\"\"\n",
    "    Use your approximate_betweenness implementation to partition a graph.\n",
    "    Unlike in class, here you will not implement this recursively. Instead,\n",
    "    just remove edges until more than one component is created, then return\n",
    "    those components.\n",
    "    That is, compute the approximate betweenness of all edges, and remove\n",
    "    them until multiple comonents are created.\n",
    "\n",
    "    You only need to compute the betweenness once.\n",
    "    If there are ties in edge betweenness, break by edge name (e.g.,\n",
    "    (('A', 'B'), 1.0) comes before (('B', 'C'), 1.0)).\n",
    "\n",
    "    Note: the original graph variable should not be modified. Instead,\n",
    "    make a copy of the original graph prior to removing edges.\n",
    "    See the Graph.copy method https://networkx.github.io/documentation/development/reference/generated/networkx.Graph.copy.html\n",
    "    Params:\n",
    "      graph.......A networkx Graph\n",
    "      max_depth...An integer representing the maximum depth to search.\n",
    "\n",
    "    Returns:\n",
    "      A list of networkx Graph objects, one per partition.\n",
    "\n",
    "    >>> components = partition_girvan_newman(example_graph(), 5)\n",
    "    >>> components = sorted(components, key=lambda x: sorted(x.nodes())[0])\n",
    "    >>> sorted(components[0].nodes())\n",
    "    ['A', 'B', 'C']\n",
    "    >>> sorted(components[1].nodes())\n",
    "    ['D', 'E', 'F', 'G']\n",
    "    \"\"\"\n",
    "    ###TODO\n",
    "    my_graph = graph.copy()\n",
    "    components = [c for c in nx.connected_component_subgraphs(my_graph)]\n",
    "    while len(components) == 1:\n",
    "        remove_edge, betweeness = sorted(approximate_betweenness(my_graph, max_depth).items(), key = lambda x : -x[1])[0]\n",
    "        my_graph.remove_edge(*remove_edge)\n",
    "        components = [c for c in nx.connected_component_subgraphs(my_graph)]\n",
    "    return(components)\n",
    "\n",
    "def get_subgraph(graph, min_degree):\n",
    "    \"\"\"Return a subgraph containing nodes whose degree is\n",
    "    greater than or equal to min_degree.\n",
    "    We'll use this in the main method to prune the original graph.\n",
    "\n",
    "    Params:\n",
    "      graph........a networkx graph\n",
    "      min_degree...degree threshold\n",
    "    Returns:\n",
    "      a networkx graph, filtered as defined above.\n",
    "\n",
    "    >>> subgraph = get_subgraph(example_graph(), 3)\n",
    "    >>> sorted(subgraph.nodes())\n",
    "    ['B', 'D', 'F']\n",
    "    >>> len(subgraph.edges())\n",
    "    2\n",
    "    \"\"\"\n",
    "    ###TODO\n",
    "    degreedict = graph.degree ()\n",
    "    return(graph.subgraph([node for node , degree in degreedict if degree >=min_degree]))\n",
    "\n",
    "\n",
    "\"\"\"\"\n",
    "Compute the normalized cut for each discovered cluster.\n",
    "I've broken this down into the three next methods.\n",
    "\"\"\"\n",
    "\n",
    "def volume(nodes, graph):\n",
    "    \"\"\"\n",
    "    Compute the volume for a list of nodes, which\n",
    "    is the number of edges in `graph` with at least one end in\n",
    "    nodes.\n",
    "    Params:\n",
    "      nodes...a list of strings for the nodes to compute the volume of.\n",
    "      graph...a networkx graph\n",
    "\n",
    "    >>> volume(['A', 'B', 'C'], example_graph())\n",
    "    4\n",
    "    \"\"\"\n",
    "    seen = {}\n",
    "    vol = 0\n",
    "    for i in nodes:\n",
    "        for j in graph.neighbors(i):\n",
    "            if ( i > j):\n",
    "                keyval = (j, i)\n",
    "            else:\n",
    "                keyval = (i,j)\n",
    "            if keyval not in seen:\n",
    "                seen[keyval] = \"True\"\n",
    "                vol  = vol + 1\n",
    "    return(vol)\n",
    "    \n",
    "\n",
    "def cut(S, T, graph):\n",
    "    \"\"\"\n",
    "    Compute the cut-set of the cut (S,T), which is\n",
    "    the set of edges that have one endpoint in S and\n",
    "    the other in T.\n",
    "    Params:\n",
    "      S.......set of nodes in first subset\n",
    "      T.......set of nodes in second subset\n",
    "      graph...networkx graph\n",
    "    Returns:\n",
    "      An int representing the cut-set.\n",
    "\n",
    "    >>> cut(['A', 'B', 'C'], ['D', 'E', 'F', 'G'], example_graph())\n",
    "    1\n",
    "    \"\"\"\n",
    "    ###TODO\n",
    "    count = 0\n",
    "    for edge in graph.edges():\n",
    "        if edge[0] in S and edge[1] in T:\n",
    "            count = count + 1\n",
    "    return (count)\n",
    "\n",
    "def norm_cut(S, T, graph):\n",
    "    \"\"\"\n",
    "    The normalized cut value for the cut S/T. (See lec06.)\n",
    "    Params:\n",
    "      S.......set of nodes in first subset\n",
    "      T.......set of nodes in second subset\n",
    "      graph...networkx graph\n",
    "    Returns:\n",
    "      An float representing the normalized cut value\n",
    "\n",
    "    \"\"\"\n",
    "    ###TODO\n",
    "    return ((cut(S,T,graph) / volume(S,graph)) + (cut(S,T,graph) / volume(T,graph)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_max_depths(graph, max_depths):\n",
    "    \"\"\"\n",
    "    In order to assess the quality of the approximate partitioning method\n",
    "    we've developed, we will run it with different values for max_depth\n",
    "    and see how it affects the norm_cut score of the resulting partitions.\n",
    "    Recall that smaller norm_cut scores correspond to better partitions.\n",
    "\n",
    "    Params:\n",
    "      graph........a networkx Graph\n",
    "      max_depths...a list of ints for the max_depth values to be passed\n",
    "                   to calls to partition_girvan_newman\n",
    "\n",
    "    Returns:\n",
    "      A list of (int, float) tuples representing the max_depth and the\n",
    "      norm_cut value obtained by the partitions returned by\n",
    "      partition_girvan_newman. See Log.txt for an example.\n",
    "    \"\"\"\n",
    "    ###TODO\n",
    "    outval = []\n",
    "    for depth in max_depths:\n",
    "        components = partition_girvan_newman(graph, depth)\n",
    "        S = components[0].nodes()\n",
    "        T = components[1].nodes()\n",
    "        \n",
    "        \n",
    "        outval.append((depth, norm_cut(S, T, graph)))\n",
    "    return outval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1.2222222222222223), (2, 0.41666666666666663)]"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_max_depths(example_graph(), [1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Link prediction\n",
    "\n",
    "# Next, we'll consider the link prediction problem. In particular,\n",
    "# we will remove 5 of the accounts that Bill Gates likes and\n",
    "# compute our accuracy at recovering those links.\n",
    "\n",
    "def make_training_graph(graph, test_node, n):\n",
    "    \"\"\"\n",
    "    To make a training graph, we need to remove n edges from the graph.\n",
    "    As in lecture, we'll assume there is a test_node for which we will\n",
    "    remove some edges. Remove the edges to the first n neighbors of\n",
    "    test_node, where the neighbors are sorted alphabetically.\n",
    "    E.g., if 'A' has neighbors 'B' and 'C', and n=1, then the edge\n",
    "    ('A', 'B') will be removed.\n",
    "\n",
    "    Be sure to *copy* the input graph prior to removing edges.\n",
    "\n",
    "    Params:\n",
    "      graph.......a networkx Graph\n",
    "      test_node...a string representing one node in the graph whose\n",
    "                  edges will be removed.\n",
    "      n...........the number of edges to remove.\n",
    "\n",
    "    Returns:\n",
    "      A *new* networkx Graph with n edges removed.\n",
    "\n",
    "    In this doctest, we remove edges for two friends of D:\n",
    "    >>> g = example_graph()\n",
    "    >>> sorted(g.neighbors('D'))\n",
    "    ['B', 'E', 'F', 'G']\n",
    "    >>> train_graph = make_training_graph(g, 'D', 2)\n",
    "    >>> sorted(train_graph.neighbors('D'))\n",
    "    ['F', 'G']\n",
    "    \"\"\"\n",
    "    ###TODO\n",
    "    new_graph = copy.deepcopy(graph)\n",
    "    for i in range(n):\n",
    "         \n",
    "        node = sorted(new_graph.neighbors(test_node))[0]\n",
    "        remove_edge = (test_node, node)\n",
    "\n",
    "        #if remove_edge not in new_graph.edges():\n",
    "         #   print(\"flg\")\n",
    "          #  remove_edge = (node, test_node)\n",
    "        new_graph.remove_edge(*remove_edge)\n",
    "    return (new_graph)\n",
    "    \n",
    "\n",
    "def jaccard(graph, node, k):\n",
    "    \"\"\"\n",
    "    Compute the k highest scoring edges to add to this node based on\n",
    "    the Jaccard similarity measure.\n",
    "    Note that we don't return scores for edges that already appear in the graph.\n",
    "\n",
    "    Params:\n",
    "      graph....a networkx graph\n",
    "      node.....a node in the graph (a string) to recommend links for.\n",
    "      k........the number of links to recommend.\n",
    "\n",
    "    Returns:\n",
    "      A list of tuples in descending order of score representing the\n",
    "      recommended new edges. Ties are broken by\n",
    "      alphabetical order of the terminal node in the edge.\n",
    "\n",
    "    In this example below, we remove edges (D, B) and (D, E) from the\n",
    "    example graph. The top two edges to add according to Jaccard are\n",
    "    (D, E), with score 0.5, and (D, A), with score 0. (Note that all the\n",
    "    other remaining edges have score 0, but 'A' is first alphabetically.)\n",
    "\n",
    "    >>> g = example_graph()\n",
    "    >>> train_graph = make_training_graph(g, 'D', 2)\n",
    "    >>> jaccard(train_graph, 'D', 2)\n",
    "    [(('D', 'E'), 0.5), (('D', 'A'), 0.0)]\n",
    "    \"\"\"\n",
    "    ###TODO\n",
    "    neighbors = set(graph.neighbors(node))\n",
    "    scores = []\n",
    "    for n in graph.nodes():\n",
    "        neighbors2 = set(graph.neighbors(n))\n",
    "        scores.append((n, len(neighbors & neighbors2) /\n",
    "                          len(neighbors | neighbors2)))\n",
    "    final_list = []\n",
    "    count = 0\n",
    "    for candidate in sorted(scores, key=lambda x: x[1], reverse=True):\n",
    "        if candidate[0] != node :\n",
    "            if (node,candidate[0]) not in graph.edges():\n",
    "                count = count + 1\n",
    "                final_list.append(((node,candidate[0]), candidate[1]))\n",
    "        if count == k:\n",
    "            break\n",
    "                \n",
    "    return (final_list)\n",
    "\n",
    "\n",
    "# One limitation of Jaccard is that it only has non-zero values for nodes two hops away.\n",
    "#\n",
    "# Implement a new link prediction function that computes the similarity between two nodes $x$ and $y$  as follows:\n",
    "#\n",
    "# $$\n",
    "# s(x,y) = \\beta^i n_{x,y,i}\n",
    "# $$\n",
    "#\n",
    "# where\n",
    "# - $\\beta \\in [0,1]$ is a user-provided parameter\n",
    "# - $i$ is the length of the shortest path from $x$ to $y$\n",
    "# - $n_{x,y,i}$ is the number of shortest paths between $x$ and $y$ with length $i$\n",
    "\n",
    "def path_score(graph, root, k, beta):\n",
    "    \"\"\"\n",
    "    Compute a new link prediction scoring function based on the shortest\n",
    "    paths between two nodes, as defined above.\n",
    "\n",
    "    Note that we don't return scores for edges that already appear in the graph.\n",
    "\n",
    "    This algorithm should have the same time complexity as bfs above.\n",
    "\n",
    "    Params:\n",
    "      graph....a networkx graph\n",
    "      root.....a node in the graph (a string) to recommend links for.\n",
    "      k........the number of links to recommend.\n",
    "      beta.....the beta parameter in the equation above.\n",
    "\n",
    "    Returns:\n",
    "      A list of tuples in descending order of score. Ties are broken by\n",
    "      alphabetical order of the terminal node in the edge.\n",
    "\n",
    "    In this example below, we remove edge (D, F) from the\n",
    "    example graph. The top two edges to add according to path_score are\n",
    "    (D, F), with score 0.5, and (D, A), with score .25. (Note that (D, C)\n",
    "    is tied with a score of .25, but (D, A) is first alphabetically.)\n",
    "\n",
    "    >>> g = example_graph()\n",
    "    >>> train_graph = g.copy()\n",
    "    >>> train_graph.remove_edge(*('D', 'F'))\n",
    "    >>> path_score(train_graph, 'D', k=4, beta=.5)\n",
    "    [(('D', 'F'), 0.5), (('D', 'A'), 0.25), (('D', 'C'), 0.25)]\n",
    "    \"\"\"\n",
    "    ###TODO\n",
    "    count = 0\n",
    "    final_list = []\n",
    "    node2distances , node2num_paths, node2parents = bfs(graph, root,5) \n",
    "    for node in sorted(node2distances.items(), reverse = True):\n",
    "        if node[0] is not root:\n",
    "            if ( root, node[0]) not in graph.edges():\n",
    "                count = count + 1\n",
    "                final_list.append(((root, node[0]), beta**node[1] * node2num_paths[node[0]]))\n",
    "        if count == k:\n",
    "            break\n",
    "    final_list.sort(key = lambda x:x[0][1])\n",
    "    final_list.sort(key = lambda x:x[0][0])\n",
    "    final_list.sort(key = lambda x:x[1], reverse = True)\n",
    "    \n",
    "    \n",
    "    return(final_list)\n",
    "\n",
    "def evaluate(predicted_edges, graph):\n",
    "    \"\"\"\n",
    "    Return the fraction of the predicted edges that exist in the graph.\n",
    "\n",
    "    Args:\n",
    "      predicted_edges...a list of edges (tuples) that are predicted to\n",
    "                        exist in this graph\n",
    "      graph.............a networkx Graph\n",
    "\n",
    "    Returns:\n",
    "      The fraction of edges in predicted_edges that exist in the graph.\n",
    "\n",
    "    In this doctest, the edge ('D', 'E') appears in the example_graph,\n",
    "    but ('D', 'A') does not, so 1/2 = 0.5\n",
    "\n",
    "    >>> evaluate([('D', 'E'), ('D', 'A')], example_graph())\n",
    "    0.5\n",
    "    \"\"\"\n",
    "    ###TODO\n",
    "    count = 0\n",
    "    for edge in predicted_edges:\n",
    "        if edge in graph.edges():\n",
    "            count = count + 1\n",
    "    return(count / len(predicted_edges))\n",
    "    pass\n",
    "\n",
    "\"\"\"\n",
    "Next, we'll download a real dataset to see how our algorithm performs.\n",
    "\"\"\"\n",
    "def download_data():\n",
    "    \"\"\"\n",
    "    Download the data. Done for you.\n",
    "    \"\"\"\n",
    "    urllib.request.urlretrieve('http://cs.iit.edu/~culotta/cs579/a1/edges.txt.gz', 'edges.txt.gz')\n",
    "\n",
    "def read_graph():\n",
    "    \"\"\" Read 'edges.txt.gz' into a networkx **undirected** graph.\n",
    "    Done for you.\n",
    "    Returns:\n",
    "      A networkx undirected graph.\n",
    "    \"\"\"\n",
    "    return nx.read_edgelist('edges.txt.gz', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph has 5062 nodes and 6060 edges\n",
      "subgraph has 712 nodes and 1710 edges\n",
      "norm_cut scores by max_depth:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-430-d87bfd530319>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-430-d87bfd530319>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m           (subgraph.order(), subgraph.number_of_edges()))\n\u001b[0;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'norm_cut scores by max_depth:'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore_max_depths\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mclusters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpartition_girvan_newman\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     print('first partition: cluster 1 has %d nodes and cluster 2 has %d nodes' %\n",
      "\u001b[1;32m<ipython-input-333-fe32354b0e82>\u001b[0m in \u001b[0;36mscore_max_depths\u001b[1;34m(graph, max_depths)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0moutval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mdepth\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmax_depths\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mcomponents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpartition_girvan_newman\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[0mS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcomponents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mT\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcomponents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-426-39809a84bdb7>\u001b[0m in \u001b[0;36mpartition_girvan_newman\u001b[1;34m(graph, max_depth)\u001b[0m\n\u001b[0;32m    273\u001b[0m     \u001b[0mcomponents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mc\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnected_component_subgraphs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmy_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    274\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomponents\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 275\u001b[1;33m         \u001b[0mremove_edge\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbetweeness\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mapproximate_betweenness\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmy_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    276\u001b[0m         \u001b[0mmy_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove_edge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mremove_edge\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    277\u001b[0m         \u001b[0mcomponents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mc\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnected_component_subgraphs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmy_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-426-39809a84bdb7>\u001b[0m in \u001b[0;36mapproximate_betweenness\u001b[1;34m(graph, max_depth)\u001b[0m\n\u001b[0;32m    213\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m         \u001b[0mnode2distances\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode2num_paths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode2parents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbfs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 215\u001b[1;33m         \u001b[0mfinalresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfinalresult\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbottom_up\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode2distances\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode2num_paths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode2parents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    216\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfinalresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m         \u001b[0mfinalresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfinalresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\collections\\__init__.py\u001b[0m in \u001b[0;36m__add__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    700\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0melem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m             \u001b[0mnewcount\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcount\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0melem\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    703\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mnewcount\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0melem\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnewcount\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    FYI: This takes ~10-15 seconds to run on my laptop.\n",
    "    \"\"\"\n",
    "    download_data()\n",
    "    graph = read_graph()\n",
    "    print('graph has %d nodes and %d edges' %\n",
    "          (graph.order(), graph.number_of_edges()))\n",
    "    subgraph = get_subgraph(graph, 2)\n",
    "    print('subgraph has %d nodes and %d edges' %\n",
    "          (subgraph.order(), subgraph.number_of_edges()))\n",
    "    print('norm_cut scores by max_depth:')\n",
    "    print(score_max_depths(subgraph, range(1,5)))\n",
    "    clusters = partition_girvan_newman(subgraph, 3)\n",
    "    print('first partition: cluster 1 has %d nodes and cluster 2 has %d nodes' %\n",
    "          (clusters[0].order(), clusters[1].order()))\n",
    "    print('cluster 2 nodes:')\n",
    "    print(clusters[1].nodes())\n",
    "\n",
    "    test_node = 'Bill Gates'\n",
    "    train_graph = make_training_graph(subgraph, test_node, 5)\n",
    "    print('train_graph has %d nodes and %d edges' %\n",
    "          (train_graph.order(), train_graph.number_of_edges()))\n",
    "\n",
    "\n",
    "    jaccard_scores = jaccard(train_graph, test_node, 5)\n",
    "    print('\\ntop jaccard scores for Bill Gates:')\n",
    "    print(jaccard_scores)\n",
    "    print('jaccard accuracy=%g' %\n",
    "          evaluate([x[0] for x in jaccard_scores], subgraph))\n",
    "\n",
    "    path_scores = path_score(train_graph, test_node, k=5, beta=.1)\n",
    "    print('\\ntop path scores for Bill Gates for beta=.1:')\n",
    "    print(path_scores)\n",
    "    print('path accuracy for beta .1=%g' %\n",
    "          evaluate([x[0] for x in path_scores], subgraph))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
